# neural-network-momentum-training
A simple implementation demonstrating how momentum enhances the training of neural networks. Includes visualizations and comparisons of standard SGD vs SGD with momentum using NumPy.
# Neural Network Training with Momentum

This repository contains a Jupyter Notebook that demonstrates how adding **momentum** to the **Stochastic Gradient Descent (SGD)** optimizer can improve the training of neural networks.

## ðŸ“˜ Notebook Overview

### File:
- `4_1_3_Training_a_Neural_Network_with_Momentum.ipynb`

### Contents:
- Manual implementation of a simple neural network using **NumPy**
- Training the network using:
  - **Standard SGD**
  - **SGD with Momentum**
- Visualization of:
  - Loss curves
  - Effect of momentum on convergence
- Analysis and comparison of training performance

## ðŸ§  Key Concepts

- **Gradient Descent**
- **Momentum Term**
- **Weight Updates**
- **Loss Function Behavior**

Momentum helps accelerate training and reduces oscillations, especially in ravine-like areas of the loss surface.

## ðŸš€ Getting Started

### Prerequisites

- Python 3.x
- Jupyter Notebook
- NumPy
- Matplotlib

### Run the Notebook

1. Clone the repository:

   ```bash
   git clone https://github.com/yourusername/neural-network-momentum-training.git
   cd neural-network-momentum-training
